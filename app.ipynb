{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, year\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sys\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Création de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Analyse des Ventes\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Création d'un exemple de données pour le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnees_test = \"\"\"\n",
    "2024-01-01 Paris Ordinateur 1200\n",
    "2024-01-02 Lyon Telephone 800\n",
    "2024-01-03 Paris Tablette 600\n",
    "2024-01-04 Marseille Ordinateur 1100\n",
    "2024-02-01 Lyon Telephone 750\n",
    "2023-02-02 Paris Ordinateur 1300\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Écriture des données de test dans un fichier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ventes.txt\", \"w\") as f:\n",
    "    f.write(donnees_test.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Total des ventes par ville\n",
    "#### Lecture du fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"ventes.txt\", sep=\" \", schema=\"date STRING, ville STRING, produit STRING, prix DOUBLE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calcul du total des ventes par ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultat Exercice 1 - Total des ventes par ville:\n",
      "+---------+------------+\n",
      "|    ville|total_ventes|\n",
      "+---------+------------+\n",
      "|     Lyon|      1550.0|\n",
      "|Marseille|      1100.0|\n",
      "|    Paris|      3100.0|\n",
      "+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_ventes_ville = df.groupBy(\"ville\") \\\n",
    "    .agg(sum(\"prix\").alias(\"total_ventes\")) \\\n",
    "    .orderBy(\"ville\")\n",
    "\n",
    "print(\"Résultat Exercice 1 - Total des ventes par ville:\")\n",
    "total_ventes_ville.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: Total des ventes par ville pour une année donnée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultat Exercice 2 - Total des ventes par ville pour 2024:\n",
      "+---------+-----------------+\n",
      "|    ville|total_ventes_2024|\n",
      "+---------+-----------------+\n",
      "|     Lyon|           1550.0|\n",
      "|Marseille|           1100.0|\n",
      "|    Paris|           1800.0|\n",
      "+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ajout d'une colonne année\n",
    "df_with_year = df.withColumn(\"annee\", year(df.date))\n",
    "\n",
    "# Fonction pour calculer le total des ventes par ville pour une année spécifique\n",
    "def calculer_ventes_annee(annee):\n",
    "    resultat = df_with_year.filter(df_with_year.annee == annee) \\\n",
    "        .groupBy(\"ville\") \\\n",
    "        .agg(sum(\"prix\").alias(f\"total_ventes_{annee}\")) \\\n",
    "        .orderBy(\"ville\")\n",
    "    return resultat\n",
    "\n",
    "# Exemple pour l'année 2024\n",
    "print(\"\\nRésultat Exercice 2 - Total des ventes par ville pour 2024:\")\n",
    "ventes_2024 = calculer_ventes_annee(2024)\n",
    "ventes_2024.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Exécution de l'exercice 1 ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Analyse des Ventes, master=local[*]) created by getOrCreate at C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9660\\2219761829.py:4 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 127\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Exécution des deux exercices\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Exécution de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexercice 1 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 127\u001b[0m \u001b[43mmain_ex1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Exécution de l\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexercice 2 ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    130\u001b[0m main_ex2(\u001b[38;5;241m2024\u001b[39m, is_local)\n",
      "Cell \u001b[1;32mIn[41], line 49\u001b[0m, in \u001b[0;36mmain_ex1\u001b[1;34m(is_local)\u001b[0m\n\u001b[0;32m     46\u001b[0m     input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs:///data/ventes.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Initialisation de Spark\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43minit_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mExercice1_VentesParVille\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Traitement des ventes\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     city_sales \u001b[38;5;241m=\u001b[39m process_sales_by_city(sc, input_path)\n",
      "Cell \u001b[1;32mIn[41], line 19\u001b[0m, in \u001b[0;36minit_spark\u001b[1;34m(app_name, master)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(app_name)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\hp\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:449\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    446\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    450\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    453\u001b[0m         \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    454\u001b[0m             currentAppName,\n\u001b[0;32m    455\u001b[0m             currentMaster,\n\u001b[0;32m    456\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction,\n\u001b[0;32m    457\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfile,\n\u001b[0;32m    458\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mlinenum,\n\u001b[0;32m    459\u001b[0m         )\n\u001b[0;32m    460\u001b[0m     )\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Analyse des Ventes, master=local[*]) created by getOrCreate at C:\\Users\\hp\\AppData\\Local\\Temp\\ipykernel_9660\\2219761829.py:4 "
     ]
    }
   ],
   "source": [
    "def create_test_file():\n",
    "    \"\"\"Crée un fichier de test avec des données d'exemple\"\"\"\n",
    "    test_data = \"\"\"2024-01-01 Paris Ordinateur 1200\n",
    "2024-01-02 Lyon Telephone 800\n",
    "2024-01-03 Paris Tablette 600\n",
    "2024-02-01 Marseille Ordinateur 1100\n",
    "2024-02-02 Lyon Telephone 750\n",
    "2023-12-25 Paris Television 2000\"\"\"\n",
    "    with open(\"sales_cluster.txt\", \"w\") as f:\n",
    "                f.write(test_data)\n",
    "\n",
    "def init_spark(app_name, master=None):\n",
    "    \"\"\"Initialise Spark en fonction du mode (local ou cluster)\"\"\"\n",
    "    if master:\n",
    "        conf = SparkConf().setAppName(app_name).setMaster(master)\n",
    "    else:\n",
    "        conf = SparkConf().setAppName(app_name)\n",
    "    \n",
    "    return SparkContext(conf=conf)\n",
    "def process_sales_by_city(sc, input_path):\n",
    "    \"\"\"Calcule le total des ventes par ville\"\"\"\n",
    "    try:\n",
    "        # Lecture du fichier\n",
    "        sales_rdd = sc.textFile(input_path)\n",
    "\n",
    "        # Transformation en paires (ville, prix)\n",
    "        city_sales = sales_rdd \\\n",
    "            .map(lambda line: line.split()) \\\n",
    "            .map(lambda fields: (fields[1], float(fields[3]))) \\\n",
    "            .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "        return city_sales\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "def main_ex1(is_local=True):\n",
    "    \"\"\"Programme principal pour l'exercice 1\"\"\"\n",
    "    if is_local:\n",
    "        # Création du fichier de test en local\n",
    "        create_test_file()\n",
    "        master = \"local[*]\"\n",
    "        input_path = \"ventes.txt\"\n",
    "    else:\n",
    "        # En mode cluster\n",
    "        master = None\n",
    "        input_path = \"hdfs:///data/ventes.txt\"\n",
    "\n",
    "    # Initialisation de Spark\n",
    "    sc = init_spark(\"Exercice1_VentesParVille\", master)\n",
    "    \n",
    "    try:\n",
    "        # Traitement des ventes\n",
    "        city_sales = process_sales_by_city(sc, input_path)\n",
    "\n",
    "        # Affichage des résultats\n",
    "        print(\"\\nRésultats des ventes totales par ville:\")\n",
    "        for city, total in city_sales.collect():\n",
    "            print(f\"Ville: {city}, Total des ventes: {total}€\")\n",
    "\n",
    "    finally:\n",
    "        sc.stop()\n",
    "\n",
    "# ================= Exercice 2 =================\n",
    "def extract_year(date_str):\n",
    "    \"\"\"Extrait l'année d'une date au format YYYY-MM-DD\"\"\"\n",
    "    try:\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').year\n",
    "    except ValueError as e:\n",
    "        print(f\"Erreur de format de date: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_sales_by_city_year(sc, input_path, target_year):\n",
    "    \"\"\"Calcule le total des ventes par ville pour une année donnée\"\"\"\n",
    "    try:\n",
    "        # Lecture du fichier\n",
    "        sales_rdd = sc.textFile(input_path)\n",
    "\n",
    "        # Broadcast de l'année cible\n",
    "        broadcast_year = sc.broadcast(target_year)\n",
    "\n",
    "        # Transformation et filtrage par année\n",
    "        yearly_sales = sales_rdd \\\n",
    "            .map(lambda line: line.split()) \\\n",
    "            .filter(lambda fields: extract_year(fields[0]) == broadcast_year.value) \\\n",
    "            .map(lambda fields: (fields[1], float(fields[3]))) \\\n",
    "            .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "        return yearly_sales\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du traitement: {str(e)}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "def main_ex2(target_year=2024, is_local=True):\n",
    "    \"\"\"Programme principal pour l'exercice 2\"\"\"\n",
    "    if is_local:\n",
    "        master = \"local[*]\"\n",
    "        input_path = \"sales_cluster.txt\"\n",
    "    else:\n",
    "        master = None\n",
    "        input_path = \"hdfs:///data/sales_cluster.txt\"\n",
    "\n",
    "    # Initialisation de Spark\n",
    "    sc = init_spark(\"Exercice2_VentesParVilleAnnee\", master)\n",
    "    \n",
    "    try:\n",
    "        # Traitement des ventes pour l'année cible\n",
    "        yearly_sales = process_sales_by_city_year(sc, input_path, target_year)\n",
    "\n",
    "        # Affichage des résultats\n",
    "        print(f\"\\nRésultats des ventes par ville pour l'année {target_year}:\")\n",
    "        for city, total in yearly_sales.collect():\n",
    "            print(f\"Ville: {city}, Total des ventes: {total}€\")\n",
    "\n",
    "    finally:\n",
    "        sc.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Par défaut, exécution en mode local\n",
    "    is_local = True\n",
    "    \n",
    "    if len(sys.argv) > 1:\n",
    "        # Si des arguments sont fournis, on suppose une exécution sur cluster\n",
    "        is_local = False\n",
    "        \n",
    "    # Exécution des deux exercices\n",
    "    print(\"=== Exécution de l'exercice 1 ===\")\n",
    "    main_ex1(is_local)\n",
    "    \n",
    "    print(\"\\n=== Exécution de l'exercice 2 ===\")\n",
    "    main_ex2(2024, is_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** Arrêt de la session Spark ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
